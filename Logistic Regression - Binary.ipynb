{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Binary\n",
    "\n",
    "Running a logistic regression with Python and Spark! \n",
    "\n",
    "Steps to follow: \n",
    "\n",
    "1. Create a Spark Session and import LogisticRegression\n",
    "2. Load data and check if it's in the format - label, features\n",
    "3. Split data into training and testing set (7:3)\n",
    "4. Create an instance of Logistic Regression \n",
    "5. Create a model by using the instance to train/fit training data \n",
    "6. Use trained model to obtain prediction results by evaluating on testing data\n",
    "7. Select label and predictions from prediction results\n",
    "8. Create evaluator instance \n",
    "9. Get accuracy by evaluating predictions and label on evaluator instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('logregdoc').getOrCreate()\n",
    "\n",
    "#Since Logistic Regression is a classification task it falls under ml.classification\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "data = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
    "\n",
    "# Check if data is formatted as label and features\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[122,123,148...|[22.6839973146669...|[0.99999999985924...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[16.8927374259317...|[0.99999995391311...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[32.4786205273090...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[18.0200716259477...|[0.99999998507266...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[28.9440252750824...|[0.99999999999973...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[21.0927052690363...|[0.99999999930887...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[23.3070269958379...|[0.99999999992451...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[26.5544262041464...|[0.99999999999706...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[23.0883641925825...|[0.99999999990606...|       0.0|\n",
      "|  0.0|(692,[150,151,152...|[22.6609374860020...|[0.99999999985596...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[14.5587799472210...|[0.99999952444363...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[15.2256857685969...|[0.99999975589954...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[26.6501795038488...|[0.99999999999733...|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[19.8695477479598...|[0.99999999765163...|       0.0|\n",
      "|  0.0|(692,[181,182,183...|[12.5424541998369...|[0.99999642826030...|       0.0|\n",
      "|  0.0|(692,[234,235,237...|[2.98036289583430...|[0.95167906215259...|       0.0|\n",
      "|  1.0|(692,[99,100,101,...|[-8.7579412249466...|[1.57183220966194...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-25.042942921169...|[1.33041789779931...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-23.567010379349...|[5.82072976773139...|       1.0|\n",
      "|  1.0|(692,[125,126,127...|[-24.876332649908...|[1.57161440269341...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing set (7:3)\n",
    "\n",
    "train, test = data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Create an instance of Logistic Regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train/fit model on training data \n",
    "\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Use trained model to obtain prediction results by evaluating on testing data\n",
    "\n",
    "predictionResults = lrModel.evaluate(test)\n",
    "\n",
    "# Show prediction results\n",
    "\n",
    "predictionResults.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema of evaluated data frame\n",
    "predictionResults.predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare the label (true values) with the prediction (predicted values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_and_labels = predictionResults.predictions.select('label','prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_and_labels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of available Evaluation Metrics- \n",
    "1. BinaryClassificationEvaluator - \n",
    "    1. areaUnderROC curve\n",
    "    2. areaUnderPR (Precision-Recall) curve   \n",
    "    \n",
    "2. MulticlassClassificationEvaluator -\n",
    "    1. f1 score\n",
    "    2. weightedPrecision score\n",
    "    3. weightedRecall score\n",
    "    4. accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluator object\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator                                  \n",
    "    \n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n",
    "\n",
    "# Pass the evaluated data frame to a evaluator\n",
    "\n",
    "evaluator.evaluate(prediction_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the area under ROC curve was 1.0. That means all observations were classified with 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
